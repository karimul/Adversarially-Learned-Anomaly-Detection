{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IGEBM_PCD_ASGLD.v2.Sinkhorn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimul/Adversarially-Learned-Anomaly-Detection/blob/master/IGEBM_PCD_ASGLD_v2_Sinkhorn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qvp-sfKR6cn"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import os\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "ROOT = \"/content/drive/My Drive/Colab Notebooks\"\r\n",
        "sample_dir = os.path.join(ROOT, 'IGEBM_PCD_ASGLD.v2.Sinkhorn')\r\n",
        "if not os.path.exists(sample_dir):\r\n",
        "    os.makedirs(sample_dir)\r\n",
        "os.chdir(sample_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE0kEApQSC6s"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7_bblP6SDYL"
      },
      "source": [
        "import math\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWI3FXtBSNQg"
      },
      "source": [
        "class ConvBNReLU(nn.Module):\r\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\r\n",
        "        if isinstance(kernel_size, tuple):\r\n",
        "            padding = (max(kernel_size) - 1) // 2\r\n",
        "        else:\r\n",
        "            padding = (kernel_size - 1) // 2\r\n",
        "\r\n",
        "        super(ConvBNReLU, self).__init__()\r\n",
        "        self.conv = nn.utils.spectral_norm(nn.Conv2d(\r\n",
        "            in_planes,\r\n",
        "            out_planes,\r\n",
        "            kernel_size,\r\n",
        "            stride,\r\n",
        "            padding,\r\n",
        "            groups=groups,\r\n",
        "            bias=False,\r\n",
        "        ))\r\n",
        "        self.bn_normal = nn.BatchNorm2d(out_planes)\r\n",
        "        self.bn_adversial = nn.BatchNorm2d(out_planes)\r\n",
        "        self.act = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "    def forward(self, x, adversial=False):\r\n",
        "        x = self.conv(x)\r\n",
        "        if adversial:\r\n",
        "            x = self.bn_adversial(x)\r\n",
        "        else:\r\n",
        "            x = self.bn_normal(x)\r\n",
        "        x = self.act(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "class StandardCNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(StandardCNN, self).__init__()\r\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\r\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\r\n",
        "\r\n",
        "        self.conv3 = nn.utils.spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\r\n",
        "        self.conv4 = nn.utils.spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\r\n",
        "\r\n",
        "        self.conv5 = nn.utils.spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\r\n",
        "        self.conv6 = nn.utils.spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\r\n",
        "\r\n",
        "        self.conv7 = nn.utils.spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\r\n",
        "\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)\r\n",
        "        self.act = nn.LeakyReLU(negative_slope=0.1, inplace=True)\r\n",
        "        self.dense = nn.utils.spectral_norm(nn.Linear(512 * 4 * 4, 1))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        x = self.act(self.conv1(x))\r\n",
        "        x = self.act(self.conv2(x))\r\n",
        "        # x = self.pool(x)\r\n",
        "        x = self.act(self.conv3(x))\r\n",
        "        x = self.act(self.conv4(x))\r\n",
        "        # x = self.pool(x)\r\n",
        "        x = self.act(self.conv5(x))\r\n",
        "        x = self.act(self.conv6(x))\r\n",
        "        # x = self.pool(x)\r\n",
        "        x = self.act(self.conv7(x))\r\n",
        "\r\n",
        "        x = self.dense(x.view(x.shape[0], -1))\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "class StandardBNCNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(StandardBNCNN, self).__init__()\r\n",
        "        self.conv1 = ConvBNReLU(3, 64)\r\n",
        "        self.conv2 = ConvBNReLU(64, 64)\r\n",
        "        self.conv3 = ConvBNReLU(64, 128)\r\n",
        "        self.conv4 = ConvBNReLU(128, 128)\r\n",
        "        self.conv5 = ConvBNReLU(128, 256)\r\n",
        "        self.conv6 = ConvBNReLU(256, 256)\r\n",
        "        self.conv7 = ConvBNReLU(256, 512)\r\n",
        "        self.conv8 = ConvBNReLU(512, 512)\r\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\r\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        self.fc = nn.utils.spectral_norm(nn.Linear(512, 1))\r\n",
        "\r\n",
        "    def forward(self, x, adversial=False):\r\n",
        "        x = self.conv1(x, adversial)\r\n",
        "        x = self.conv2(x, adversial)\r\n",
        "        x = self.max_pool(x)\r\n",
        "        x = self.conv3(x, adversial)\r\n",
        "        x = self.conv4(x, adversial)\r\n",
        "        x = self.max_pool(x)\r\n",
        "        x = self.conv5(x, adversial)\r\n",
        "        x = self.conv6(x, adversial)\r\n",
        "        x = self.max_pool(x)\r\n",
        "        x = self.conv7(x, adversial)\r\n",
        "        x = self.conv8(x, adversial)\r\n",
        "        # (B, 512, 4, 4)\r\n",
        "\r\n",
        "        x = self.avg_pool(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv3uauOLSQfE"
      },
      "source": [
        "class SampleReplayBuffer:\r\n",
        "    def __init__(self, batch_size, buffer_length=10000, data_size=(3, 32, 32)):\r\n",
        "        self.buffer_length = buffer_length\r\n",
        "        self.data_size = data_size\r\n",
        "        self.buffer = torch.rand((self.buffer_length,) + self.data_size)\r\n",
        "        self.index = 0\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.cpu = torch.device(\"cpu\")\r\n",
        "        self.gpu = torch.device(\"cuda:0\")\r\n",
        "\r\n",
        "    def sample(self):\r\n",
        "        indices = torch.randint(low=0, high=self.buffer_length, size=(self.batch_size,))\r\n",
        "        return self.buffer[indices].to(self.gpu, non_blocking=True)\r\n",
        "\r\n",
        "    def add_sample(self, x):\r\n",
        "        if self.index + self.batch_size >= self.buffer_length:\r\n",
        "            end = self.buffer_length - self.index\r\n",
        "            self.buffer[self.index : self.index + end] = x[:end].to(\r\n",
        "                self.cpu, non_blocking=True\r\n",
        "            )\r\n",
        "\r\n",
        "            start = self.batch_size - end\r\n",
        "            self.buffer[:start] = x[end:].to(self.cpu, non_blocking=True)\r\n",
        "        else:\r\n",
        "            self.buffer[self.index : self.index + self.batch_size] = x.to(\r\n",
        "                self.cpu, non_blocking=True\r\n",
        "            )\r\n",
        "\r\n",
        "        self.index = (self.index + self.batch_size) % self.buffer_length\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSVM3wHGSh5c"
      },
      "source": [
        "class SGLDTrainer:\r\n",
        "    def __init__(self, dataset=''):\r\n",
        "        self.batch_size = 128\r\n",
        "        self.last_epoch = -1\r\n",
        "        self.epochs = 500\r\n",
        "        self.num_workers = 3\r\n",
        "        self.smoothness_scale = 1.0\r\n",
        "\r\n",
        "        if dataset == 'CelebA':\r\n",
        "            transform = transforms.Compose([\r\n",
        "                # resize\r\n",
        "                transforms.Resize(32),\r\n",
        "                # center-crop\r\n",
        "                transforms.CenterCrop(32),\r\n",
        "                # to-tensor\r\n",
        "                transforms.ToTensor()\r\n",
        "            ])\r\n",
        "            trainset = torchvision.datasets.CelebA(\r\n",
        "                root=\"./data\", split='Train', download=True, transform=transform\r\n",
        "            )\r\n",
        "        elif dataset == 'AFHQ':\r\n",
        "            transform = transforms.Compose([\r\n",
        "                # resize\r\n",
        "                transforms.Resize(32),\r\n",
        "                # center-crop\r\n",
        "                transforms.CenterCrop(32),\r\n",
        "                # to-tensor\r\n",
        "                transforms.ToTensor()\r\n",
        "            ])\r\n",
        "            trainset = torchvision.datasets.ImageFolder(root='./data/AFHQ', transform=transform)\r\n",
        "        else:\r\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\r\n",
        "            trainset = torchvision.datasets.CIFAR10(\r\n",
        "                root=\"./data\", train=True, download=True, transform=transform\r\n",
        "            )\r\n",
        "\r\n",
        "        self.trainloader = torch.utils.data.DataLoader(\r\n",
        "            trainset,\r\n",
        "            batch_size=self.batch_size,\r\n",
        "            shuffle=True,\r\n",
        "            num_workers=self.num_workers,\r\n",
        "            drop_last=True,\r\n",
        "        )\r\n",
        "        # testset = torchvision.datasets.CIFAR10(\r\n",
        "        #     root=\"./data\", train=False, download=True, transform=transform\r\n",
        "        # )\r\n",
        "        # self.testloader = torch.utils.data.DataLoader(\r\n",
        "        #     testset,\r\n",
        "        #     batch_size=self.batch_size,\r\n",
        "        #     shuffle=False,\r\n",
        "        #     num_workers=self.num_workers,\r\n",
        "        # )\r\n",
        "\r\n",
        "        self.device = torch.device(\"cuda:0\")\r\n",
        "        self.model = StandardCNN()\r\n",
        "        self.model.to(self.device)\r\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001, betas=[0.0, 0.999])\r\n",
        "        self.step = 0\r\n",
        "\r\n",
        "        self.buffer_sample_rate = 0.95\r\n",
        "        self.data_size = (3, 32, 32)\r\n",
        "        self.sample_replay_buffer = SampleReplayBuffer(\r\n",
        "            self.batch_size, data_size=self.data_size\r\n",
        "        )\r\n",
        "\r\n",
        "        self.dynamics_steps = 60\r\n",
        "        self.step_size = 10\r\n",
        "        self.noise_scale = 0.005\r\n",
        "\r\n",
        "        self.noise = 0.1\r\n",
        "        self.momentum = 0.9\r\n",
        "        self.eps = 1e-6\r\n",
        "\r\n",
        "        self.writer = SummaryWriter()\r\n",
        "\r\n",
        "        self.FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\r\n",
        "        self.LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "\r\n",
        "        for epoch in range(self.last_epoch + 1, self.epochs):\r\n",
        "            average_loss = 0.0\r\n",
        "            for i, (x, _) in enumerate(self.trainloader, 0):\r\n",
        "                output = self.process_batch(x)\r\n",
        "                loss = output[\"loss\"].item()\r\n",
        "                average_loss += (loss - average_loss) / (i + 1)               \r\n",
        "\r\n",
        "                if self.step % 50 == 0:\r\n",
        "                    print(f\"[{i+1}/{len(self.trainloader)}] -- loss {loss:.5f} -- avg. loss {average_loss:.5f}\")\r\n",
        "                    self.log({\"average_loss\": average_loss, **output})\r\n",
        "\r\n",
        "                self.step += 1\r\n",
        "\r\n",
        "            print(f\"\\nEpoch {epoch} started...\")\r\n",
        "            checkpoint = {\r\n",
        "                'state_dict': self.model.state_dict(),\r\n",
        "                'optimizer': self.optimizer.state_dict(),\r\n",
        "                'step' : self.step,\r\n",
        "                'epoch' : epoch\r\n",
        "            }\r\n",
        "            # torch.save(self.model.state_dict(), f\"./model{epoch}.pt\")\r\n",
        "            torch.save(checkpoint, f\"./model{epoch}.pt\")\r\n",
        "\r\n",
        "    def process_batch(self, x):\r\n",
        "\r\n",
        "        x = x.to(self.device, non_blocking=True)\r\n",
        "\r\n",
        "        sample = self.get_sample()\r\n",
        "\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        losses = self.compute_loss(positive_examples=x, negative_examples=sample)\r\n",
        "        losses[\"loss\"].backward()\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        return {\"positive_examples\": x, \"negative_examples\": sample, **losses}\r\n",
        "\r\n",
        "    def compute_loss(self, positive_examples, negative_examples):\r\n",
        "        # negative_examples size (128, 3, 32, 32)\r\n",
        "        positive_energy = self.model(positive_examples)\r\n",
        "        negative_energy = self.model(negative_examples) # size (128,1)\r\n",
        "\r\n",
        "        # print(\"positive_energy size\", positive_energy.size())\r\n",
        "\r\n",
        "        # maximum_likelihood_loss = positive_energy - negative_energy\r\n",
        "        smoothness_loss = (\r\n",
        "            positive_energy ** 2 + negative_energy ** 2\r\n",
        "        ) * self.smoothness_scale\r\n",
        "\r\n",
        "        # print(\"smoothness_loss\",smoothness_loss)\r\n",
        "        # total_loss = maximum_likelihood_loss + smoothness_loss\r\n",
        "\r\n",
        "        # Sinkhorn parameters\r\n",
        "        epsilon = 0.01\r\n",
        "        niter = 100\r\n",
        "\r\n",
        "        total_loss = self.sinkhorn_loss(negative_energy,positive_energy,epsilon,niter) \r\n",
        "        # print(\"total_loss\",total_loss)\r\n",
        "\r\n",
        "        return {\r\n",
        "            \"loss\": total_loss.mean(),\r\n",
        "            \"positive_energy\": positive_energy,\r\n",
        "            \"negative_energy\": negative_energy,\r\n",
        "            # \"maximum_likelihood_loss\": maximum_likelihood_loss,\r\n",
        "            # \"smoothness_loss\": smoothness_loss,\r\n",
        "        }\r\n",
        "\r\n",
        "    def sinkhorn_loss(self, x, y, epsilon, niter):\r\n",
        "        \"\"\"\r\n",
        "        Given two emprical measures with n points each with locations x and y\r\n",
        "        outputs an approximation of the OT cost with regularization parameter epsilon\r\n",
        "        niter is the max. number of steps in sinkhorn loop\r\n",
        "        \"\"\"\r\n",
        "        def _cost_matrix(x, y, p=2):\r\n",
        "            \"Returns the matrix of $|x_i-y_j|^p$.\"\r\n",
        "            x_col = x.unsqueeze(-2)\r\n",
        "            y_lin = y.unsqueeze(-3)\r\n",
        "            c = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)\r\n",
        "            return c\r\n",
        "\r\n",
        "        # The Sinkhorn algorithm takes as input three variables :\r\n",
        "        C = _cost_matrix(x, y)  # Wasserstein cost function\r\n",
        "        x_points = x.shape[-2]\r\n",
        "        y_points = y.shape[-2]\r\n",
        "\r\n",
        "        # print(\"x size\", x.size())\r\n",
        "\r\n",
        "        if x.dim() == 2:\r\n",
        "            batch_size = 1\r\n",
        "        else:\r\n",
        "            batch_size = x.shape[0]\r\n",
        "        \r\n",
        "        # both marginals are fixed with equal weights\r\n",
        "        mu = torch.empty(batch_size, x_points, dtype=torch.float,\r\n",
        "                         requires_grad=False).fill_(1.0 / x_points).squeeze().type(self.FloatTensor)\r\n",
        "        nu = torch.empty(batch_size, y_points, dtype=torch.float,\r\n",
        "                         requires_grad=False).fill_(1.0 / y_points).squeeze().type(self.FloatTensor)\r\n",
        "        # size (128,32)\r\n",
        "        # print(\"mu size\", mu.size())\r\n",
        "        # Parameters of the Sinkhorn algorithm.\r\n",
        "        rho = 1  # (.5) **2          # unbalanced transport\r\n",
        "        tau = -.8  # nesterov-like acceleration\r\n",
        "        lam = rho / (rho + epsilon)  # Update exponent\r\n",
        "        thresh = 1e-9  # stopping criterion\r\n",
        "\r\n",
        "        # Elementary operations .....................................................................\r\n",
        "        def ave(u, u1):\r\n",
        "            \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\r\n",
        "            return tau * u + (1 - tau) * u1\r\n",
        "\r\n",
        "        def M(u, v):\r\n",
        "            \"Modified cost for logarithmic updates\"\r\n",
        "            \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\r\n",
        "            # print(\"u\", u.unsqueeze(-1))\r\n",
        "            # print(\"v\", v.unsqueeze(-2))\r\n",
        "            # print(\"-C\",-C)\r\n",
        "            return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / epsilon\r\n",
        "\r\n",
        "        # Actual Sinkhorn loop ......................................................................\r\n",
        "        err = 0.\r\n",
        "        u = torch.zeros_like(mu)\r\n",
        "        v = torch.zeros_like(nu)\r\n",
        "        \r\n",
        "        actual_nits = 0  # to check if algorithm terminates because of threshold or max iterations reached\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            # Sinkhorn iterations\r\n",
        "            for _ in range(niter):\r\n",
        "                u1 = u  # useful to check the update\r\n",
        "                u = epsilon * (torch.log(mu + 1e-8) - torch.logsumexp(M(u, v), dim=-1)) + u\r\n",
        "                v = epsilon * (torch.log(nu + 1e-8) - torch.logsumexp(M(u, v).transpose(-2, -1), dim=-1)) + v\r\n",
        "                err = (u - u1).abs().sum(-1).mean()\r\n",
        "\r\n",
        "                actual_nits += 1\r\n",
        "                if err.item() < thresh:\r\n",
        "                    break\r\n",
        "            U, V = u, v\r\n",
        "        pi = torch.exp(M(U, V))  # Transport plan pi = diag(a)*K*diag(b)\r\n",
        "        cost = torch.sum(pi * C, dim=(-2, -1))  # Sinkhorn cost\r\n",
        "\r\n",
        "        return cost\r\n",
        "            \r\n",
        "    def get_sample(self):\r\n",
        "        neg_img = self.get_initial_sample()\r\n",
        "        neg_img.requires_grad = True\r\n",
        "        self.model.eval()\r\n",
        "\r\n",
        "        # Intialize mean and variance to zero\r\n",
        "        mean = torch.zeros_like(neg_img.data)\r\n",
        "        std = torch.zeros_like(neg_img.data)\r\n",
        "        weight_decay = 5e-4\r\n",
        "        for i in range(self.dynamics_steps):\r\n",
        "            # Getting mean,std at previous step\r\n",
        "            old_mean = mean.clone()\r\n",
        "            old_std = std.clone()\r\n",
        "\r\n",
        "            noise = torch.normal(mean=old_mean, std=old_std)\r\n",
        "            neg_img.data.add(self.noise, noise)\r\n",
        "            if weight_decay != 0:\r\n",
        "                neg_img.data.add_(weight_decay, neg_img.data)        \r\n",
        "            \r\n",
        "            energy = self.model(neg_img)\r\n",
        "            energy.backward(torch.ones_like(energy))\r\n",
        "\r\n",
        "            # Updating mean\r\n",
        "            mean = mean.mul(self.momentum).add(neg_img.data)\r\n",
        "\r\n",
        "            # Updating std\r\n",
        "            part_var1 = neg_img.data.add(-old_mean)\r\n",
        "            part_var2 = neg_img.data.add(-mean)\r\n",
        "\r\n",
        "            new_std = torch.pow(old_std,2).mul(self.momentum).addcmul(1,part_var1,part_var2).add(self.eps)                \r\n",
        "            new_std = torch.pow(torch.abs_(new_std),1/2)\r\n",
        "            std.add_(-1,std).add_(new_std)\r\n",
        "\r\n",
        "            neg_img.grad.data.clamp_(-0.01, 0.01)           \r\n",
        " \r\n",
        "            neg_img.data.add_(-self.step_size, neg_img.grad.data)\r\n",
        " \r\n",
        "            neg_img.grad.detach_()\r\n",
        "            neg_img.grad.zero_()\r\n",
        " \r\n",
        "            neg_img.data.clamp_(0, 1)\r\n",
        "\r\n",
        "            # if self.step % 50 == 0:\r\n",
        "                # print(energy.detach().mean(), torch.std_mean(neg_img.detach()))\r\n",
        "\r\n",
        "        sample = neg_img.detach()\r\n",
        "        self.sample_replay_buffer.add_sample(sample)        \r\n",
        "        self.model.train()\r\n",
        "        self.model.zero_grad()\r\n",
        "\r\n",
        "        return sample\r\n",
        "\r\n",
        "    def get_initial_sample(self):\r\n",
        "        if torch.rand(1) > self.buffer_sample_rate:\r\n",
        "            return torch.rand((self.batch_size,) + self.data_size, device=self.device)\r\n",
        "        else:\r\n",
        "            return self.sample_replay_buffer.sample()\r\n",
        "\r\n",
        "    def load_checkpoint(self):\r\n",
        "        checkpoint = torch.load(f\"./model28.pt\")\r\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\r\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\r\n",
        "        self.step = checkpoint['step']\r\n",
        "        self.last_epoch = checkpoint['epoch']         \r\n",
        "\r\n",
        "    def log(self, data):\r\n",
        "\r\n",
        "        self.writer.add_scalar(\"average loss\", data[\"average_loss\"], self.step)\r\n",
        "        self.writer.add_scalar(\"batch loss\", data[\"loss\"], self.step)\r\n",
        "\r\n",
        "        std, mean = torch.std_mean(data[\"positive_energy\"])\r\n",
        "        self.writer.add_scalar(\"positive energy mean\", mean, self.step)\r\n",
        "        self.writer.add_scalar(\"positive energy std\", std, self.step)\r\n",
        "\r\n",
        "        std, mean = torch.std_mean(data[\"negative_energy\"])\r\n",
        "        self.writer.add_scalar(\"negative energy mean\", mean, self.step)\r\n",
        "        self.writer.add_scalar(\"negative energy std\", std, self.step)\r\n",
        "\r\n",
        "        # self.writer.add_scalar(\r\n",
        "        #     \"smoothness loss\", data[\"smoothness_loss\"].mean(), self.step\r\n",
        "        # )\r\n",
        "\r\n",
        "        self.writer.add_images(\r\n",
        "            \"positive examples\", data[\"positive_examples\"], self.step\r\n",
        "        )\r\n",
        "        self.writer.add_images(\r\n",
        "            \"negative examples\", data[\"negative_examples\"], self.step\r\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KHUzO4idBCI"
      },
      "source": [
        "tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQDXcmc-SyY1"
      },
      "source": [
        "trainer = SGLDTrainer('')\r\n",
        "# trainer.load_checkpoint()\r\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}